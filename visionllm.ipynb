{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2312.16171 (referenced from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     load_dotenv\n\u001b[0;32m      4\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapi_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benaaf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Set up OpenAI client\n",
    "def configure():\n",
    "    load_dotenv\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "client = OpenAI(api_key=os.getenv('api_key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_frames(video_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % int(fps) == 0:\n",
    "            timestamp = frame_count / fps\n",
    "            timestamp_str = str(timedelta(seconds=int(timestamp)))\n",
    "            frame_filename = f\"frame_{timestamp_str.replace(':', '_')}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "            print(f\"Saved frame at {timestamp_str}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Total frames in video: {frame_count}\")\n",
    "    print(f\"Frames saved (1 per second): {saved_count}\")\n",
    "    return saved_count\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def process_image(image_path, prompt=\"Describe this image in one sentence.\"):\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes images accurately and concisely. The person that you are assisting is a blind person. You will say DANGER if the person if you detect any cars, obtacles, potholes, people, and other dangeroud things in close vicinity\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def process_frames(frames_folder, output_csv, max_frames=2000):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Timestamp', 'Description'])\n",
    "        \n",
    "        # Sort the frames by their timestamp\n",
    "        frames = sorted(os.listdir(frames_folder))\n",
    "        \n",
    "        # Process only the first max_frames\n",
    "        for frame_filename in frames[:max_frames]:\n",
    "            frame_path = os.path.join(frames_folder, frame_filename)\n",
    "            timestamp_str = frame_filename.split('_')[1].replace('.jpg', '').replace('_', ':')\n",
    "            \n",
    "            description = process_image(frame_path)\n",
    "            \n",
    "            csvwriter.writerow([timestamp_str, description])\n",
    "            \n",
    "            print(f\"Timestamp: {timestamp_str}\")\n",
    "            print(f\"Description: {description}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        if len(frames) > max_frames:\n",
    "            print(f\"Note: Only processed the first {max_frames} frames out of {len(frames)} total frames.\")\n",
    "\n",
    "def main(video_path, output_csv, max_frames=10):\n",
    "    configure()\n",
    "    frames_folder = \"extracted_frames\"\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    print(\"Extracting frames from video...\")\n",
    "    extract_and_save_frames(video_path, frames_folder)\n",
    "    \n",
    "    # Step 2: Process frames with LLM\n",
    "    print(f\"Processing the first {max_frames} frames with LLM...\")\n",
    "    process_frames(frames_folder, output_csv, max_frames)\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_csv}\")\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % int(fps) == 0:\n",
    "            timestamp = frame_count / fps\n",
    "            timestamp_str = str(timedelta(seconds=int(timestamp)))\n",
    "            frame_filename = f\"frame_{timestamp_str.replace(':', '_')}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "            print(f\"Saved frame at {timestamp_str}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Total frames in video: {frame_count}\")\n",
    "    print(f\"Frames saved (1 per second): {saved_count}\")\n",
    "    return saved_count\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def process_image(image_path, prompt=\"Describe this image in one sentence.\"):\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes images accurately and concisely. The person that you are assisting is a blind person. You will say DANGER if the person if you detect any cars, obtacles, potholes, people, and other dangeroud things in close vicinity\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def process_frames(frames_folder, output_csv, max_frames=10):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Timestamp', 'Description'])\n",
    "        \n",
    "        # Sort the frames by their timestamp\n",
    "        frames = sorted(os.listdir(frames_folder))\n",
    "        \n",
    "        # Process only the first max_frames\n",
    "        for frame_filename in frames[:max_frames]:\n",
    "            frame_path = os.path.join(frames_folder, frame_filename)\n",
    "            timestamp_str = frame_filename.split('_')[1].replace('.jpg', '').replace('_', ':')\n",
    "            \n",
    "            description = process_image(frame_path)\n",
    "            \n",
    "            csvwriter.writerow([timestamp_str, description])\n",
    "            \n",
    "            print(f\"Timestamp: {timestamp_str}\")\n",
    "            print(f\"Description: {description}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        if len(frames) > max_frames:\n",
    "            print(f\"Note: Only processed the first {max_frames} frames out of {len(frames)} total frames.\")\n",
    "\n",
    "def main(video_path, output_csv, max_frames=100):\n",
    "    frames_folder = \"extracted_frames\"\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    print(\"Extracting frames from video...\")\n",
    "    extract_and_save_frames(video_path, frames_folder)\n",
    "    \n",
    "    # Step 2: Process frames with LLM\n",
    "    print(f\"Processing the first {max_frames} frames with LLM...\")\n",
    "    process_frames(frames_folder, output_csv, max_frames)\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "\n",
    "video_path = \"./video.mp4\"\n",
    "output_csv = \"./video_descriptions.csv\"\n",
    "max_frames_to_process = 100\n",
    "\n",
    "main(video_path, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m     31\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./video_descriptions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[43msummarize_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36msummarize_csv\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_csv\u001b[39m(csv_path):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Combine all descriptions into a single text\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     combined_descriptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def summarize_csv(csv_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine all descriptions into a single text\n",
    "    combined_descriptions = \" \".join(df['Description'].tolist())\n",
    "    \n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"Summarize the following descriptions: {combined_descriptions}\"\n",
    "    \n",
    "    # Get the summary from the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text accurately and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    \n",
    "    print(\"Summary of CSV Descriptions:\")\n",
    "    print(summary)\n",
    "    # Save the summary to a text file\n",
    "    with open(\"summary.txt\", \"w\") as f:\n",
    "        f.write(summary)\n",
    "    return summary\n",
    "\n",
    "# Usage\n",
    "csv_path = \"./video_descriptions.csv\"\n",
    "summarize_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
